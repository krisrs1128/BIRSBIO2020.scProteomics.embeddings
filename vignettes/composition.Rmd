---
title: "Disentangling Composition and Spatial Effects"
author: Kris Sankaran^[ksankaran@wisc.edu]
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Disentangling Composition and Spatial Effects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  K: 20
---

* Question we answer in this vignette: to what extent can spatial features from
  MIBI-ToF data be identified from the composition of cells.
* More direct question: We know that if a sample has only one cell type, then it
  cannot have mixing. We want to push this thinking further.
    - Can you determine spatial properties (like the amount of mixing between
      cell types, the cell density of the sample) from just the composition of
      cells that are present?
    - This is an interesting question because
      + Cells are known to be

```{r}
library("SummarizedExperiment")
library("dplyr")
library("forcats")
library("ggplot2")
library("igraph")
library("reshape2")
library("tibble")
library("viridis")
library("BIRSBIO2020.scProteomics.embeddings")
theme_set(theme_bw() + theme(panel.grid = element_blank()))
```

```{r, message = FALSE, warning = FALSE}
data_dir <- file.path(Sys.getenv("HOME"), "Data")
data_paths <- download_data(data_dir)
loaded_ <- load_mibi(data_dir)
subsample <- spatial_subsample(loaded_$tiffs, loaded_$mibi)
ims <- subsample$ims
mibi_sce <- subsample$exper
```

* get cluster the proteins, and extract compositions from each sample
* plot this against the tumor intrusion
* now calculate my own spatial statistics.
  - neighborhood entropy
  - neighborhood size
* relate summaries of these spatial statistics to composition
  - if not totally predictive, then the latent idea is worthwhile
  - otherwise, it's a surprise to the community

```{r}
cell_clusters <- kmeans(t(assay(mibi_sce)), params$K)
props <- sample_proportions(colData(mibi_sce)$SampleID, cell_clusters$cluster)
mprops <- data.frame(props)
```

```{r}
cd <- colData(mibi_sce) %>%
  as.data.frame() %>%
  tidyr::unite(scell, SampleID, cellLabelInImage, remove = FALSE) %>%
  mutate(
    cell_type = cell_type(mibi_sce),
    cell_group = fct_lump(cell_type, prop = 0.05),
    SampleID = factor(SampleID, rownames(props))
  )

cd_samp <- cd %>%
  group_by(SampleID) %>%
  summarise_all(function(x) x[1] ) %>%
  mutate(SampleID = factor(SampleID, rownames(props)))

cluster_ids <- setNames(cell_clusters$cluster, cd$scell)
```

These are example sample-level cluster compositions, based entirely on antigen
information (ignoring spatial information).
```{r}
ggplot(mprops %>% left_join(cd_samp)) +
  geom_tile(aes(x = SampleID, y = cluster, fill = sqrt(Freq))) +
  facet_grid(. ~ GRADE, scale="free_x", space="free_x") +
  scale_fill_gradient(low = "white", high = "black") +
  theme(legend.position = "bottom")
```

Extract local entropy and neighborhood sizes.

```{r spatial_stat, message = FALSE, warning = FALSE}
sample_names <- as.character(cd_samp$SampleID)
graphs <- list()
for (i in seq_along(sample_names)) {
  print(sprintf("graph %s/%s", i, length(sample_names)))
  poly <- polygonize(ims[[sample_names[i]]]) %>%
    filter(cellLabelInImage > 1)
  graphs[[i]] <- extract_graph(poly)
}

spatial <- list()
for (i in seq_along(sample_names)) {
  print(sprintf("spatial stats %s/%s", i, length(sample_names)))
  SG <- subgraphs(graphs[[i]])
  ptrn <- paste0("^", sample_names[i], "_")
  clusters_ <- cluster_ids[grepl(ptrn, names(cluster_ids))]
  names(clusters_) <- gsub(ptrn, "", names(clusters_))

  spatial[[sample_names[i]]] <- tibble(
    scell = paste0(sample_names[i], "_", names(V(graphs[[i]]))),
    entropy = entropies(SG, clusters_),
    avg_dists = avg_dists(SG)
  )
}
```

```{r plot_spatial}
spatial <- bind_rows(spatial, .id = "SampleID") %>%
  mutate(SampleID = factor(SampleID, levels(mprops$SampleID)))

spatial_samp <- spatial %>%
  inner_join(cd) %>%
  group_by(SampleID) %>%
  summarise(
    me = mean(entropy),
    sde = sd(entropy),
    mdist = mean(avg_dists),
    sdist = sd(avg_dists),
    .groups = "drop"
  ) %>%
  left_join(cd_samp)

spatial_cell <- spatial %>%
  tidyr::separate(scell, c("SampleID", "cellLabelInImage")) %>%
  mutate(
    cellLabelInImage = as.numeric(cellLabelInImage)
  ) %>%
  left_join(cd)
```

```{r}
## overall histograms
ggplot(spatial_cell) +
  geom_histogram(aes(x = entropy, fill = as.factor(TIL_score))) +
  scale_fill_brewer(palette = "Greens", na.value = "grey") +
  facet_grid(cell_group ~ ., scale = "free") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0)
  )

ggplot(spatial_cell) +
  geom_histogram(aes(x = avg_dists, fill = as.factor(TIL_score))) +
  scale_fill_brewer(palette = "Greens", na.value = "grey") +
  facet_grid(cell_group ~ ., scale = "free") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0)
  )

slev <- spatial_samp %>%
  dplyr::select(SampleID, mdist) %>%
  arrange(mdist) %>%
  .[["SampleID"]]

ggplot(spatial_cell %>%
       mutate(SampleID = factor(SampleID, levels = slev))
       ) +
  geom_point(
    aes(x = avg_dists, y = entropy, col = cell_group),
    size = 0.5, alpha = 0.8
  ) +
  facet_wrap(~ SampleID) +
  scale_fill_brewer(palette = "Set2") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0),
    panel.spacing = unit(0, "cm")
  )
```

This is how each individual differs in terms of the cell-label entropy of and
average pairwise distance of their 5-nearest neighborhoods.

```{r}
## Sample level plots
ggplot(spatial_samp) +
  geom_point(
    aes(x = me, y = Survival_days_capped_2016.1.1),
    size = 4
  ) +
  geom_point(
    aes(x = me, y = Survival_days_capped_2016.1.1, col = as.factor(TIL_score)),
    size = 2
  ) +
  scale_color_brewer(palette = "Greens", na.value = "grey") +
  theme(legend.position = "bottom")

ggplot(spatial_samp) +
  geom_point(
    aes(x = mdist, y = Survival_days_capped_2016.1.1),
    size = 4
  ) +
  geom_point(
    aes(x = mdist, y = Survival_days_capped_2016.1.1, col = as.factor(TIL_score)),
    size = 2
  ) +
  scale_color_brewer(palette = "Greens", na.value = "grey") +
  theme(legend.position = "bottom")

## Cell level plots
ggplot(spatial_cell) +
  geom_jitter(
    aes(x = TIL_score, y = entropy, col = as.factor(SampleID)),
    size = 0.8, alpha = 0.8
  ) +
  theme(legend.position = "none")

ggplot(spatial_cell) +
  geom_jitter(
    aes(x = TIL_score, y = avg_dists, col = as.factor(SampleID)),
    size = 0.8, alpha = 0.8
  ) +
  theme(legend.position = "none")
```

* Can we predict properties of the sample-wise entropy distribution based on
  just the cell compositions?
* Is there something about the survival that isn't captured entirely by the
  composition information?

```{r}
spatial_samp <- spatial_samp %>%
  mutate(SampleID = factor(SampleID, levels(mprops$SampleID)))

spatial_comp <- spatial_samp %>%
  dplyr::select(SampleID, me, sde, mdist, sdist, TIL_score) %>%
  left_join(mprops) %>%
  dcast(SampleID + me + sde + mdist + sdist + TIL_score ~ cluster)

x <- spatial_comp %>%
  dplyr::select(matches("[0-9]+")) %>%
  as.matrix() %>%
  sqrt()
```

```{r}
y <- as.numeric(scale(spatial_comp$me))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)
```

It looks like quite a bit, but not all, of the variation in average entropy, can
be explained by cell composition. What about the average neighborhood size or
standard errors of entropy and neighborhood size?

```{r}
y <- as.numeric(scale(spatial_comp$mdist))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)

y <- as.numeric(scale(spatial_comp$sde))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)

y <- as.numeric(scale(spatial_comp$sdist))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)
```

It seems like average pairwise distance is predictable, though not quite as much
as entropy. The standard errors are not predictable using the linear model, but
seem okay using the random forest (which I find an odd result...)

What about the TIL score?

```{r}
y <- as.numeric(scale(spatial_comp$TIL_score))
keep_ix <- !is.na(y)
sum(keep_ix)

fits <- fit_wrapper(x[keep_ix, ], y[keep_ix])
plot_fits(x[keep_ix, ], y[keep_ix], fits$glmnet, fits$rf)
```

There are so many missing values in TIL score, that I'm not sure whether we're
actually learning anything meaningful here. It may be the case that the entropy
and neighborhood size features are predictable, but not the TIL score? I don't
have an explanation for how that could happen, though.

# Interpreting Models

```{r}
y <- as.numeric(scale(spatial_comp$me))
fits <- fit_wrapper(x, y)
```

First, let's study the output of the glmnet model, even though it's performance is lower.

```{r}
plot(fits$glmnet$glmnet.fit)
beta_hat <- coef(fits$glmnet$glmnet.fit)[, 10]
imp_ix <- order(abs(beta_hat), decreasing = TRUE)
mx <- data.frame(x = x[, imp_ix[1:10] - 1], y = y) %>%
  melt(id.vars = c("y"))

ggplot(mx) +
  geom_point(aes(x = value, y = y)) +
  facet_wrap(~ variable)

y_order <- order(y)
image(t(cbind(y[y_order] / 4, x[y_order, imp_ix[1:10] - 1])))
```

How can we interpret the associated clusters?

```{r}
z <- cell_clusters$centers
mcenters <- melt(z, varnames = c("cluster", "protein")) %>%
  mutate(
    protein = factor(protein, colnames(z)[hclust(dist(t(z), method="manhattan"))$order]),
    cluster = factor(cluster, hclust(dist(z, method = "manhattan"))$order),
    )

mcenters <- mcenters %>%
  left_join(data.frame(
      cluster = factor(0:20, levels = c(0, levels(mcenters$cluster))),
      beta = beta_hat
    ))

ggplot(mcenters) +
  geom_bar(
    aes(x = protein, y = value, fill = beta),
    stat = "identity"
  ) +
  scale_fill_viridis() +
  scale_y_continuous(limits = c(-2, 6), oob = scales::squish) +
  facet_wrap(~ cluster) +
  theme(axis.text.x = element_text(angle = -90, hjust = 0))

ggplot(mcenters %>% filter(beta != 0)) +
  geom_bar(
    aes(x = protein, y = value, fill = beta),
    stat = "identity"
  ) +
  scale_fill_viridis() +
  scale_y_continuous(limits = c(-2, 6), oob = scales::squish) +
  facet_grid(cluster ~ .) +
  theme(axis.text.x = element_text(angle = -90, hjust = 0))
```

We can make something similar for the random forest model.

# Regressing proxy response

An easy way to tell whether there is additional structure from one source
relative to another is to see whether the ability to predict some independently
interesting variable changes when you include that source. This is the
table-wise analog of variable importance. In principle, we could multitask
regress on to several phenotypic variables, but for now, we'll focus on
survival.

There seem to be two general ways to approach this,
* Predict using the two tables separately, then together. See to what extent the
  underlying information is orthogonal. If the information is completely
  redundant, there is no benefit by combining the separate sources.
* Compute the residual of one source, regressing out the other. This is somehow
  the "leftover" structure, and can also be used for prediction.

We'll try both approaches here. First, predicting separately, then together.

```{r}
combined_x <- spatial_comp %>%
  dplyr::select(-SampleID, -TIL_score) %>%
  mutate_at(vars(matches("[0-9]+")), sqrt) %>%
  as.matrix()

library("survival")
library("glmnet")
library("randomForestSRC")
keep_ix <- !is.na(spatial_samp$Survival_days_capped_2016.1.1)
y <- Surv(spatial_samp$Survival_days_capped_2016.1.1[keep_ix], spatial_samp$Censored[keep_ix])

surv_glmnet <- cv.glmnet(scale(combined_x[keep_ix, ]), y, family = "cox")
plot(surv_glmnet)
plot(surv_glmnet$glmnet.fit)

surv_rf <- rfsrc(y ~ . , data = data.frame(combined_x[keep_ix, ], y = y), mtry=20)
plot(surv_rf) # this model is hopeless
```

So, it doesn't seem like survival is so easy to predict, using this information.
So, let's reorient, and try to predict relevant groups of phenotypic
information. To do this, we'll convert the phenotypic metadata into something we
can do dimensionality reduction on. But first, is there any correlation between
the x matrix we've constructed and the characteristics of the people?

```{r}
fits <- vector(length = ncol(cd_samp), mode = "list")
for (j in seq_along(cd_samp)) {
  print(j)

  y <- cd_samp[[j]]
  fam <- "gaussian"
  if (!is.numeric(y)) {
    fam <- "multinomial"
    y <- y %>%
      fct_explicit_na() %>%
      fct_lump(prop = .1)

    for (rare in c("Other", "NULL", "POSITIVE FOR A DELETERIOUS MUTATION")) {
      if (sum(y == rare) < 5) {
        y[y == rare] <- "(Missing)"
      }
    }
    if (sum(y == "(Missing)") < 5) {
      y[y == "(Missing)"] <- "POSITIVE"
    }

    y <- droplevels(y)
    if (nlevels(y) == 1) next
  } else {
    if (var(y, na.rm = T) == 0) next
    y[is.na(y)] <- median(y, na.rm = T)
    y <- scale(y)
  }


  fits[[j]] <- cv.glmnet(combined_x[keep_ix, ], y[keep_ix], family=fam)
}

names(fits) <- colnames(cd_samp)
fits
```

Weirdly enough, we seem to be able to predict the year of diagnosis. We can also
predict the donor number (not really surprising). Other than that, the only
variable I'm really able to predict is TIL score, which makes sense.

```{r}
plot(fits$YEAR)
image(coef(fits$YEAR$glmnet.fit)) # a bunch of these expression values are correlated. Maybe different types of cancers collected in different years
```

Apparently this "using known phenotype" approach to assessing complementary
information doesn't seem to be that promising (we can't predict anything
meaningful other than TIL Score). We'll also spend more time deriving new
features.
