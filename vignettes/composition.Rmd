---
title: "Disentangling Composition and Spatial Effects"
author: Kris Sankaran^[ksankaran@wisc.edu]
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Disentangling Composition and Spatial Effects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  K: 20
---

* Question we answer in this vignette: to what extent can spatial features from
  MIBI-ToF data be identified from the composition of cells.
* More direct question: We know that if a sample has only one cell type, then it
  cannot have mixing. We want to push this thinking further.
    - Can you determine spatial properties (like the amount of mixing between
      cell types, the cell density of the sample) from just the composition of
      cells that are present?
    - This is an interesting question because
      + Spatial properties are known to be predictive of certain disease
        characteristics, and can now be directly interrogated using technology
        like MIBI-ToF
      + Cell type can be directly measured using cheaper masstag technologies

* Our approach has two parts,
  - FIrst we use MIBI-TOF spatial information to extract spatial features of interest
  - We cluster the cells according to protein expression values
  - We predict spatial properties based on proportions of cell clusters
* Reason for clustering according to expression values is that we may want to
  use a more granular collection of clusters than are available from prior
  analysis / which are designed for human consumption.

First, let's load the libraries needed in this vignette.

```{r}
library("SummarizedExperiment")
library("dplyr")
library("forcats")
library("ggplot2")
library("igraph")
library("reshape2")
library("tibble")
library("viridis")
library("BIRSBIO2020.scProteomics.embeddings")
theme_set(theme_bw() + theme(panel.grid = element_blank()))
```

Now we'll download data for analysis, if it's not already present.

```{r, message = FALSE, warning = FALSE}
data_dir <- file.path(Sys.getenv("HOME"), "Data")
data_paths <- download_data(data_dir)
loaded_ <- load_mibi(data_dir)
subsample <- spatial_subsample(loaded_$tiffs, loaded_$mibi)
ims <- subsample$ims
mibi_sce <- subsample$exper
```

* get cluster the proteins, and extract compositions from each sample
* cell_clusters is the output of `kmeans` on the original protein expression
  matrix, and mprops summarizes how many of each cluster each sample has

```{r}
cell_clusters <- kmeans(t(assay(mibi_sce)), params$K)
props <- sample_proportions(colData(mibi_sce)$SampleID, cell_clusters$cluster)
mprops <- data.frame(props)
```

Next, we join the cluster information into high-level sample information.
cd_samp has colData summarized at the individual patient level, rather than one
row for each cell.

```{r}
cd <- colData(mibi_sce) %>%
  as.data.frame() %>%
  tidyr::unite(scell, SampleID, cellLabelInImage, remove = FALSE) %>%
  mutate(
    cell_type = cell_type(mibi_sce),
    cell_group = fct_lump(cell_type, prop = 0.05),
    SampleID = factor(SampleID, rownames(props))
  )

cd_samp <- cd %>%
  group_by(SampleID) %>%
  summarise_all(function(x) x[1] ) %>%
  mutate(SampleID = factor(SampleID, rownames(props)))

cluster_ids <- setNames(cell_clusters$cluster, cd$scell)
```

These are example sample-level cluster compositions, based entirely on antigen
information (ignoring spatial information).
```{r}
ggplot(mprops %>% left_join(cd_samp)) +
  geom_tile(aes(x = SampleID, y = cluster, fill = sqrt(Freq))) +
  facet_grid(. ~ GRADE, scale="free_x", space="free_x") +
  scale_fill_gradient(low = "white", high = "black") +
  theme(legend.position = "bottom")
```

* Next we calculate spatial statistics using the raster data (appropriately
  transformed into graphs).
* Extract local entropy and neighborhood sizes.
  - By local entropy, we mean the entropy of cluster proportion vectors within
    each cell-centric neighborhood in the sample's graph

```{r spatial_stat, message = FALSE, warning = FALSE}
sample_names <- as.character(cd_samp$SampleID)
graphs <- list()
for (i in seq_along(sample_names)) {
  print(sprintf("graph %s/%s", i, length(sample_names)))
  poly <- polygonize(ims[[sample_names[i]]]) %>%
    filter(cellLabelInImage > 1)
  graphs[[i]] <- extract_graph(poly)
}

spatial <- list()
for (i in seq_along(sample_names)) {
  print(sprintf("spatial stats %s/%s", i, length(sample_names)))
  SG <- subgraphs(graphs[[i]])
  ptrn <- paste0("^", sample_names[i], "_")
  clusters_ <- cluster_ids[grepl(ptrn, names(cluster_ids))]
  names(clusters_) <- gsub(ptrn, "", names(clusters_))

  spatial[[sample_names[i]]] <- tibble(
    scell = paste0(sample_names[i], "_", names(V(graphs[[i]]))),
    entropy = entropies(SG, clusters_),
    avg_dists = avg_dists(SG)
  )
}
```

Now we join in these spatial statistics with the sample-level summaries we
computed before.

```{r plot_spatial}
spatial <- bind_rows(spatial, .id = "SampleID") %>%
  mutate(SampleID = factor(SampleID, levels(mprops$SampleID)))

spatial_samp <- spatial %>%
  inner_join(cd) %>%
  group_by(SampleID) %>%
  summarise(
    me = mean(entropy),
    sde = sd(entropy),
    mdist = mean(avg_dists),
    sdist = sd(avg_dists),
    .groups = "drop"
  ) %>%
  left_join(cd_samp)

spatial_cell <- spatial %>%
  tidyr::separate(scell, c("SampleID", "cellLabelInImage")) %>%
  mutate(
    cellLabelInImage = as.numeric(cellLabelInImage)
  ) %>%
  left_join(cd)
```

These figures give us a sense of how the new spatial statistics are associated
with known sample characteristics. For example, when entropy is large {something
happens to the TIL score}.

```{r}
## overall histograms
ggplot(spatial_cell) +
  geom_histogram(aes(x = entropy, fill = as.factor(TIL_score))) +
  scale_fill_brewer(palette = "Greens", na.value = "grey") +
  facet_grid(cell_group ~ ., scale = "free") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0)
  )

ggplot(spatial_cell) +
  geom_histogram(aes(x = avg_dists, fill = as.factor(TIL_score))) +
  scale_fill_brewer(palette = "Greens", na.value = "grey") +
  facet_grid(cell_group ~ ., scale = "free") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0)
  )

slev <- spatial_samp %>%
  dplyr::select(SampleID, mdist) %>%
  arrange(mdist) %>%
  .[["SampleID"]]

ggplot(spatial_cell %>% mutate(SampleID = factor(SampleID, levels = slev))) +
  geom_point(
    aes(x = avg_dists, y = entropy, col = cell_group),
    size = 0.5, alpha = 0.8
  ) +
  facet_wrap(~ SampleID) +
  scale_fill_brewer(palette = "Set2") +
  theme(
    legend.position = "bottom",
    strip.text.y = element_text(angle = 0),
    panel.spacing = unit(0, "cm")
  )
```

This is just more exploratory analysis of the derived spatial features.

This is how each individual differs in terms of the cell-label entropy of and
average pairwise distance of their 5-nearest neighborhoods.

```{r}
## Sample level plots
ggplot(spatial_samp) +
  geom_point(
    aes(x = me, y = Survival_days_capped_2016.1.1),
    size = 4
  ) +
  geom_point(
    aes(x = me, y = Survival_days_capped_2016.1.1, col = as.factor(TIL_score)),
    size = 2
  ) +
  scale_color_brewer(palette = "Greens", na.value = "grey") +
  theme(legend.position = "bottom")

ggplot(spatial_samp) +
  geom_point(
    aes(x = mdist, y = Survival_days_capped_2016.1.1),
    size = 4
  ) +
  geom_point(
    aes(x = mdist, y = Survival_days_capped_2016.1.1, col = as.factor(TIL_score)),
    size = 2
  ) +
  scale_color_brewer(palette = "Greens", na.value = "grey") +
  theme(legend.position = "bottom")

## Cell level plots
ggplot(spatial_cell) +
  geom_jitter(
    aes(x = TIL_score, y = entropy, col = as.factor(SampleID)),
    size = 0.8, alpha = 0.8
  ) +
  theme(legend.position = "none")

ggplot(spatial_cell) +
  geom_jitter(
    aes(x = TIL_score, y = avg_dists, col = as.factor(SampleID)),
    size = 0.8, alpha = 0.8
  ) +
  theme(legend.position = "none")
```

* Can we predict properties of the sample-wise entropy distribution based on
  just the cell compositions?
* Is there something about the survival that isn't captured entirely by the
  composition information?
* First, we prepare covariates and response data. Our covariates are the
  (square-root transformed) cluster mixture proportions associated with each
  sample. Remember that these clusters are computed without reference to any
  spatial information -- they are in principle recoverable from masstag
  technologies alone.

```{r}
spatial_samp <- spatial_samp %>%
  mutate(SampleID = factor(SampleID, levels(mprops$SampleID)))

spatial_comp <- spatial_samp %>%
  dplyr::select(SampleID, me, sde, mdist, sdist, TIL_score) %>%
  left_join(mprops) %>%
  dcast(SampleID + me + sde + mdist + sdist + TIL_score ~ cluster)

x <- spatial_comp %>%
  dplyr::select(matches("[0-9]+")) %>%
  as.matrix() %>%
  sqrt()
```

As our first response, we'll consider the average cell-centric entropy.

```{r}
y <- as.numeric(scale(spatial_comp$me))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)
```

It looks like quite a bit, but not all, of the variation in average entropy, can
be explained by cell composition. What about the average neighborhood size or
standard errors of entropy and neighborhood size?

```{r}
y <- as.numeric(scale(spatial_comp$mdist))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)

y <- as.numeric(scale(spatial_comp$sde))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)

y <- as.numeric(scale(spatial_comp$sdist))
fits <- fit_wrapper(x, y)
plot_fits(x, y, fits$glmnet, fits$rf)
```

It seems like average pairwise distance is predictable, though not quite as much
as entropy. The standard errors are not predictable using the linear model, but
seem okay using the random forest (which I find an odd result...)

What about the TIL score?

```{r}
y <- as.numeric(scale(spatial_comp$TIL_score))
keep_ix <- !is.na(y)
sum(keep_ix)

fits <- fit_wrapper(x[keep_ix, ], y[keep_ix])
plot_fits(x[keep_ix, ], y[keep_ix], fits$glmnet, fits$rf)
```

There are so many missing values in TIL score, that I'm not sure whether we're
actually learning anything meaningful here. It may be the case that the entropy
and neighborhood size features are predictable, but not the TIL score? I don't
have an explanation for how that could happen, though.

# Interpreting Models

Now that we have established {}, we want to dig a bit more deeply into how these
fitted models are structured.

```{r}
y <- as.numeric(scale(spatial_comp$me))
fits <- fit_wrapper(x, y)
```

First, let's study the output of the glmnet model, even though it's performance is lower.

```{r}
plot(fits$glmnet$glmnet.fit)
beta_hat <- coef(fits$glmnet$glmnet.fit)[, 10]
imp_ix <- order(abs(beta_hat), decreasing = TRUE)
mx <- data.frame(x = x[, imp_ix[1:10] - 1], y = y) %>%
  melt(id.vars = c("y"))

ggplot(mx) +
  geom_point(aes(x = value, y = y)) +
  facet_wrap(~ variable)

y_order <- order(y)
image(t(cbind(y[y_order] / 4, x[y_order, imp_ix[1:10] - 1])))
```

It seems that....

How can we interpret the associated clusters?

```{r}
z <- cell_clusters$centers
mcenters <- melt(z, varnames = c("cluster", "protein")) %>%
  mutate(
    protein = factor(protein, colnames(z)[hclust(dist(t(z), method="manhattan"))$order]),
    cluster = factor(cluster, hclust(dist(z, method = "manhattan"))$order),
    )

mcenters <- mcenters %>%
  left_join(data.frame(
      cluster = factor(0:20, levels = c(0, levels(mcenters$cluster))),
      beta = beta_hat
    ))

ggplot(mcenters) +
  geom_bar(
    aes(x = protein, y = value, fill = beta),
    stat = "identity"
  ) +
  scale_fill_viridis() +
  scale_y_continuous(limits = c(-2, 6), oob = scales::squish) +
  facet_wrap(~ cluster) +
  theme(axis.text.x = element_text(angle = -90, hjust = 0))

ggplot(mcenters %>% filter(beta != 0)) +
  geom_bar(
    aes(x = protein, y = value, fill = beta),
    stat = "identity"
  ) +
  scale_fill_viridis() +
  scale_y_continuous(limits = c(-2, 6), oob = scales::squish) +
  facet_grid(cluster ~ .) +
  theme(axis.text.x = element_text(angle = -90, hjust = 0))
```

We can make something similar for the random forest model.

# Regressing proxy response

An easy way to tell whether there is additional structure from one source
relative to another is to see whether the ability to predict some independently
interesting variable changes when you include that source. This is the
table-wise analog of variable importance. In principle, we could multitask
regress on to several phenotypic variables, but for now, we'll focus on
survival.

There seem to be two general ways to approach this,
* Predict using the two tables separately, then together. See to what extent the
  underlying information is orthogonal. If the information is completely
  redundant, there is no benefit by combining the separate sources.
* Compute the residual of one source, regressing out the other. This is somehow
  the "leftover" structure, and can also be used for prediction.

We'll try both approaches here. First, predicting separately, then together.

```{r}
combined_x <- spatial_comp %>%
  dplyr::select(-SampleID, -TIL_score) %>%
  mutate_at(vars(matches("[0-9]+")), sqrt) %>%
  as.matrix()

library("survival")
library("glmnet")
library("randomForestSRC")
keep_ix <- !is.na(spatial_samp$Survival_days_capped_2016.1.1)
y <- Surv(spatial_samp$Survival_days_capped_2016.1.1[keep_ix], spatial_samp$Censored[keep_ix])

surv_glmnet <- cv.glmnet(scale(combined_x[keep_ix, ]), y, family = "cox")
plot(surv_glmnet)
plot(surv_glmnet$glmnet.fit)

surv_rf <- rfsrc(y ~ . , data = data.frame(combined_x[keep_ix, ], y = y), mtry=20)
plot(surv_rf) # this model is hopeless
```

So, it doesn't seem like survival is so easy to predict, using this information.
So, let's reorient, and try to predict relevant groups of phenotypic
information. To do this, we'll convert the phenotypic metadata into something we
can do dimensionality reduction on. But first, is there any correlation between
the x matrix we've constructed and the characteristics of the people?

```{r}
fits <- vector(length = ncol(cd_samp), mode = "list")
for (j in seq_along(cd_samp)) {
  print(j)

  y <- cd_samp[[j]]
  fam <- "gaussian"
  if (!is.numeric(y)) {
    fam <- "multinomial"
    y <- y %>%
      fct_explicit_na() %>%
      fct_lump(prop = .1)

    for (rare in c("Other", "NULL", "POSITIVE FOR A DELETERIOUS MUTATION")) {
      if (sum(y == rare) < 5) {
        y[y == rare] <- "(Missing)"
      }
    }
    if (sum(y == "(Missing)") < 5) {
      y[y == "(Missing)"] <- "POSITIVE"
    }

    y <- droplevels(y)
    if (nlevels(y) == 1) next
  } else {
    if (var(y, na.rm = T) == 0) next
    y[is.na(y)] <- median(y, na.rm = T)
    y <- scale(y)
  }


  fits[[j]] <- cv.glmnet(combined_x[keep_ix, ], y[keep_ix], family=fam)
}

names(fits) <- colnames(cd_samp)
fits
```

Weirdly enough, we seem to be able to predict the year of diagnosis. We can also
predict the donor number (not really surprising). Other than that, the only
variable I'm really able to predict is TIL score, which makes sense.

```{r}
plot(fits$YEAR)
image(coef(fits$YEAR$glmnet.fit)) # a bunch of these expression values are correlated. Maybe different types of cancers collected in different years
```

Apparently this "using known phenotype" approach to assessing complementary
information doesn't seem to be that promising (we can't predict anything
meaningful other than TIL Score). We'll also spend more time deriving new
features.
